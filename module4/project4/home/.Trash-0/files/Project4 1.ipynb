{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda create -n py27 python=2.7 anaconda\n",
    "source activate py27\n",
    "conda install -c Quantopian zipline\n",
    "QUANDL_API_KEY=<API KEY> zipline ingest -b quandl # this takes hours but you only do this once forever\n",
    "conda install networkx==1.9.1   # conda zipline version doesn't enforce this\n",
    "conda install -c cvxgrp cvxpy libgcc\n",
    "pip install git+https://github.com/quantopian/alphalens\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Universe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research_tools import run_pipeline, get_symbols, get_pricing\n",
    "from zipline.pipeline.factors import Returns, AverageDollarVolume\n",
    "from zipline.pipeline import Pipeline\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500)\n",
    "\n",
    "# a pipeline screen controls what is returned **not** what is calculated; a mask controls what is calculated\n",
    "p = Pipeline(screen=universe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get this just for one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2016-01-05'  # must be a valid trading day\n",
    "end_date = '2016-01-05'    # must be a valid trading day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df = run_pipeline(p, start_date, end_date)\n",
    "end = time.time()\n",
    "print('Compute time (s): %f' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df.index.get_level_values(1).values.tolist()\n",
    "tickers[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = get_pricing(tickers, start_date='2011-01-05', end_date='2016-01-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = dat.pct_change()[1:].fillna(0)\n",
    "n_pos = len(rets.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create, Fit, and Test Statistical Risk Model\n",
    "\n",
    "Fit a model with 20 latent risk factors. Return:\n",
    "    - factor betas\n",
    "    - factor covariance matrix\n",
    "    - idiosyncratic variance matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class RiskModelPCA():\n",
    "    \n",
    "    ANN_FACTOR = 252\n",
    "    \n",
    "    def __init__(self, num_factors):\n",
    "        self._num_factors = num_factors\n",
    "        self.num_stocks_ = None\n",
    "        self.factor_betas_ = None\n",
    "        self.factor_returns_ = None\n",
    "        self.common_returns_ = None\n",
    "        self.residuals_ = None\n",
    "        self.factor_cov_matrix_ = None\n",
    "        self.idio_var_matrix_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "\n",
    "    def fit(self, returns):\n",
    "        self.num_stocks_ = len(returns.columns)\n",
    "        mod = PCA(n_components=self._num_factors, svd_solver='full')\n",
    "        mod.fit(returns)\n",
    "        \n",
    "        self.factor_betas_ = pd.DataFrame(\n",
    "            data=mod.components_.T,\n",
    "            index=returns.columns\n",
    "        )\n",
    "        \n",
    "        self.factor_returns_ = pd.DataFrame(\n",
    "            data=mod.transform(rets),\n",
    "            index=returns.index\n",
    "        )\n",
    "        \n",
    "        self.explained_variance_ratio_ = mod.explained_variance_ratio_\n",
    "        \n",
    "        self.common_returns_ = pd.DataFrame(\n",
    "            data=np.dot(self.factor_returns_, self.factor_betas_.T),\n",
    "            index=returns.index\n",
    "        )\n",
    "        self.common_returns_.columns = returns.columns\n",
    "        \n",
    "        self.residuals_ = (returns - self.common_returns_)\n",
    "        \n",
    "        self.factor_cov_matrix_ = np.diag(\n",
    "            self.factor_returns_.var(axis=0, ddof=1)*RiskModelPCA.ANN_FACTOR\n",
    "        )\n",
    "        \n",
    "        self.idio_var_matrix_ = pd.DataFrame(\n",
    "            data=np.diag(np.var(self.residuals_))*RiskModelPCA.ANN_FACTOR,\n",
    "            index=returns.columns\n",
    "        )\n",
    "        \n",
    "        self.idio_var_vector_ = pd.DataFrame(\n",
    "            data=np.diag(self.idio_var_matrix_.values),\n",
    "            index=returns.columns\n",
    "        )\n",
    "        \n",
    "        self.idio_var_matrix_.columns = index=returns.columns\n",
    "\n",
    "    def get_factor_exposures(self, weights):\n",
    "        F = self.factor_betas_.loc[weights.index]\n",
    "        return F.T.dot(weights)\n",
    "\n",
    "    def predict(self, weights):\n",
    "        \"\"\" Calculates expected portfolio risk as sqrt(h'XFX'h + h'Sh).\n",
    "            This will fail if your portfolio has asset weights not in the risk model\"\"\"\n",
    "        all_assets = pd.DataFrame(\n",
    "            data=np.repeat(0, self.num_stocks_),\n",
    "            index=self.factor_betas_.index)\n",
    "        all_assets.loc[weights.index] = weights\n",
    "        \n",
    "            \n",
    "        h = all_assets\n",
    "        X = self.factor_betas_\n",
    "        F = self.factor_cov_matrix_\n",
    "        S = self.idio_var_matrix_\n",
    "        \n",
    "        return np.sqrt(h.T.dot(X).dot(F).dot(X.T).dot(h) + h.T.dot(S).dot(h))[0].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = RiskModelPCA(20)\n",
    "rm.fit(rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The % of Variance Explained by Each Factor\n",
    "\n",
    "You will see that the first factor dominates. The precise defintion of each factor in a latent model is unknown, however we can guess at the likely intepretation. What do you think is the best definition for this first factor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(20), rm.explained_variance_ratio_);\n",
    "plt.title('% of Variance Explained by Each Factor');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Common Factor Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.factor_returns_.loc[:,0:5].cumsum().plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict One Period Forward Portfolio Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_portfolio = [tickers[i] for i in np.random.choice(len(tickers), 4).tolist()]\n",
    "sample_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(data=[0.25,0.25,0.25,0.25], index=sample_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.predict(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Risk Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random 20-day period\n",
    "# Predict risk for each of 20 days; cumulate the predicted variance\n",
    "# Run Risk-Ratio Statistic from Mahdavan paper p. 13\n",
    "# np.sqrt(np.pi/2.0)*(1.0/T)*np.sum(s/fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Alpha Factor(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ics_scheme import Sector\n",
    "from zipline.pipeline.data import USEquityPricing\n",
    "from zipline.pipeline.factors import CustomFactor, SimpleMovingAverage\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500)\n",
    "p = Pipeline(screen=universe)\n",
    "\n",
    "# This is a momentum factor\n",
    "# Hypothesis: higher past 12-month (252 days) returns are proportional to future return\n",
    "factor_1 = (\n",
    "    Returns(window_length=252, mask=universe).\n",
    "    demean(groupby=Sector()).\n",
    "    rank().\n",
    "    zscore()\n",
    ")\n",
    "\n",
    "# This is a mean reversion factor\n",
    "# Hypothesis: short-term outperformers(underperformers) compared to their sector will revert \n",
    "factor_2 = (\n",
    "    -Returns(window_length=5, mask=universe).\n",
    "    demean(groupby=Sector()).\n",
    "    rank().\n",
    "    zscore()\n",
    ")\n",
    "\n",
    "# This is a mean reversion factor\n",
    "# Hypothesis: short-term outperformers(underperformers) to their sector will revert \n",
    "# With the addition that we smooth the factor output\n",
    "factor_3 = (\n",
    "    SimpleMovingAverage(inputs=[factor_2], window_length=5).\n",
    "    rank().\n",
    "    zscore()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class CTO(Returns):\n",
    "    \"\"\"\n",
    "    Computes the overnight return, per hypothesis from\n",
    "    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010\n",
    "    \"\"\"\n",
    "    window_length=2\n",
    "\n",
    "    inputs = [USEquityPricing.open, USEquityPricing.close]\n",
    "    \n",
    "    # The opens and closes matrix is 2 rows x N assets, with the most recent at the bottom.\n",
    "    # As such, opens[-1] is the most recent open, and closes[0] is the earlier close\n",
    "    \n",
    "    def compute(self, today, assets, out, opens, closes):\n",
    "        out[:] = (opens[-1] - closes[0]) / closes[0]\n",
    "\n",
    "class TrailingOvernightReturns(Returns):\n",
    "    \"\"\"\n",
    "    Sum of trailing 1m O/N returns\n",
    "    \"\"\"\n",
    "    window_safe = True\n",
    "    window_length = 5\n",
    "    \n",
    "    inputs = [CTO(mask=universe)]\n",
    "    \n",
    "    def compute(self, today, asset_ids, out, cto):\n",
    "        out[:] = np.nansum(cto, axis=0)        \n",
    "\n",
    "factor_4 = (\n",
    "    TrailingOvernightReturns().\n",
    "    rank().\n",
    "    zscore()\n",
    ")\n",
    "\n",
    "factor_5 = (\n",
    "    SimpleMovingAverage(inputs=[factor_4], window_length=5).\n",
    "    rank().\n",
    "    zscore()\n",
    ")\n",
    "\n",
    "        \n",
    "p.add(factor_1, 'Momentum_1YR')\n",
    "p.add(factor_2, 'Mean_Reversion_5Day_Sector_Neutral')\n",
    "p.add(factor_3, 'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\n",
    "p.add(factor_4, 'Overnight_Sentiment')\n",
    "p.add(factor_5, 'Overnight_Sentiment_Smoothed')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practice to inspect the DAG to ensure everything looks as you would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.show_graph(format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2014-01-06'  # must be a valid trading day\n",
    "end_date = '2016-01-05'    # must be a valid trading day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df = run_pipeline(p, start_date, end_date)\n",
    "end = time.time()\n",
    "print('Compute time (s): %f' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Date Alignment\n",
    "\n",
    "When pipeline returns with a date of, e.g., `2016-01-07` this includes data that would be known as of before the **market open** on `2016-01-07`. As such, if you ask for `USEP.close.latest` it will return the closing price from the day before and label the date `2016-01-07`. All factor values assume to be run prior to the open on the labeled day with data known before that point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate Alpha Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = df.index.levels[1].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get pricing data in order to calculate returns and factor returns. As stated above, the date labeled for the pipeline output is using data as of the day before (t-1). The pricing data we are getting uses the close price as of the labeled date (t). What this means in practice, is we will evaluate the alpha factor return as if we calculated the factor before the market open, but executed on it at that day's close. This is called a **delay 1** setting and is conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing = get_pricing(\n",
    "    assets,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    'close'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pricing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format alpha factors and pricing for `alphalens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_names = df.columns\n",
    "factor_data = {}\n",
    "\n",
    "start_time = time.clock()\n",
    "for factor in factor_names:\n",
    "    print(\"Formatting factor data for: \" + factor)\n",
    "    factor_data[factor] = al.utils.get_clean_factor_and_forward_returns(\n",
    "        factor=df[factor],\n",
    "        prices=pricing,\n",
    "        periods=[1]\n",
    "    )\n",
    "end_time = time.clock()\n",
    "print(\"Time to get arrange factor data: %.2f secs\" % (end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Factor-Weight Returns for each Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = []\n",
    "\n",
    "start_time = time.clock()\n",
    "for i, factor in enumerate(factor_names):\n",
    "    ls = al.performance.factor_returns(factor_data[factor])\n",
    "    ls.columns = [factor]\n",
    "    ls_factor_returns.append(ls)\n",
    "end_time = time.clock()\n",
    "print(\"Time to generate long/short returns: %.2f secs\" % (end_time - start_time))\n",
    "\n",
    "df_ls_factor_returns = pd.concat(ls_factor_returns, axis=1)\n",
    "(1+df_ls_factor_returns).cumprod().plot(title='Factor Returns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally this looks ok... \"up and to the right\".\n",
    "\n",
    "## Quantile Analysis\n",
    "\n",
    "It is not enough to look just at the factor weighted return. A good alpha is also monotonic in quantiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = []\n",
    "\n",
    "start_time = time.clock()\n",
    "for i, factor in enumerate(factor_names):\n",
    "    qr = al.performance.mean_return_by_quantile(factor_data[factor])\n",
    "    qr[0].columns = [factor]\n",
    "    qr_factor_returns.append(qr[0])\n",
    "    #print(qr)\n",
    "end_time = time.clock()\n",
    "print(\"Time to generate quantile returns: %.2f secs\" % (end_time - start_time))\n",
    "\n",
    "df_qr_factor_returns = pd.concat(qr_factor_returns, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(10000*df_qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(4,2),\n",
    "    figsize=(14, 14),\n",
    "    legend=False,\n",
    "    title='Alphas Comparison: Basis Points Per Day per Quantile'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "- None of these alphas are **strictly monotonic**; this should lead you to question why this is? Further research and refinement of the alphas needs to be done. What is it about these alphas that leads to the highest ranking stocks in all alphas execpt MR 5D smoothed to *not* perform the best.\n",
    "- The majority of the return is coming from the **short side** in all these alphas. The negative return in quintile 1 is very large in all alphas. This could also a cause for concern becuase when you short stocks, you need to locate the short; shorts can be expensive or not available at all.\n",
    "- If you look at the magnitude of the return spread (i.e., Q1 minus Q5), we are working with daily returns in the 0.03%, i.e., **3 basis points**, neighborhood *before all transaction costs, shorting costs, etc.*. Assuming 252 days in a year, thats 7.56% return annualized. Transaction costs may cut this in half. As such, it should be clear that these alphas can only survive in an institutional setting and that leverage will likely need to be applied in order to acheive an attractive return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcuate the Sharpe Ratio of the Alphas\n",
    "\n",
    "Generally, a Sharpe Ratio of near 1.0 or higher is an acceptable single alpha for this universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sharpe Ratio (Annualized; Entire Period)\")\n",
    "pd.DataFrame(data=16*df_ls_factor_returns.mean()/df_ls_factor_returns.std(), columns=['Sharpe Ratio']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnover Analysis\n",
    "\n",
    "Without doing a full and formal backtest, we can analyze how stable the alphas are over time. Stability in this sense means that from period to period, the alpha ranks do not change much. Since trading is costly, we always perfer, all other things being equal, that the ranks do not change significantly per period. We can measure this with the **factor rank autocorrelation (FRA)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_FRA = []\n",
    "\n",
    "start_time = time.clock()\n",
    "for i, factor in enumerate(factor_names):\n",
    "    print(\"Calculating the FRA for: \" + factor)\n",
    "    ls = al.performance.factor_rank_autocorrelation(factor_data[factor]).to_frame()\n",
    "    ls.columns = [factor]\n",
    "    ls_FRA.append(ls)\n",
    "end_time = time.clock()\n",
    "print(\"Time to generate FRAs: %.2f secs\" % (end_time - start_time))\n",
    "df_ls_FRA = pd.concat(ls_FRA, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ls_FRA.plot(title=\"Factor Rank Autocorrelation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Important!\n",
    "\n",
    "What do you notice about the comparision of the Sharpe Ratios, performance curves, and FRAs for the two mean reversion factors?\n",
    "\n",
    "Answer: the Sharpe ratios and performance curves are almost identical, but the FRA is much higher for the \"Smoothed\" factor. This means that the smoothed factor will have much lower trading turnover in practice and is a much preferrable factor. The smoothing gives you a turnover reduction effectively for free.\n",
    "\n",
    "What do you think would happen if we smooth the momentum factor?\n",
    "\n",
    "Answer: the FRA is very close to 1.0 meaning the factor ranks are very stable. This makes sense since this factor is the trailing 12-month return; as one day passed, the cumulative 12-month return does not change much. As such, we should not expect any increase in FRA nor any improvement in the factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Combined Alpha Vector\n",
    "\n",
    "To use these alphas in a portfolio, we need to combine them somehow so we get a single score per stock. This is a area where machine learning can be very helpful. In this module, however, we will take the simplest approach of combination: simply averaging the scores from each alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_factors = factor_names[[1, 2, 4]]\n",
    "print(selected_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alpha_vector'] = df[selected_factors].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it All Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = df[['alpha_vector']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the alpha vector for a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_vector = alphas.loc[df.index.get_level_values(0)[-1]]\n",
    "alpha_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calculate Optimal Portfolio Constrained by Risk Model\n",
    "\n",
    "You have an alpha model and a risk model. Generally you want to trade as close as possible to the alpha model but limiting your risk as measured by the risk model. Optimization can help here.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{h}{\\text{maximize}}\n",
    "& & \\alpha^T h + \\lambda\\|h\\|_2\\\\\n",
    "& \\text{subject to}\n",
    "& & h^T XFX'h + h'Sh \\leq b \\\\\n",
    "&&& X^Th \\preceq k_{\\text{max}} \\\\\n",
    "&&& X^Th \\succeq k_{\\text{min}} \\\\\n",
    "&&& h^T\\mathbb{1} = 0 \\\\\n",
    "&&& \\|h\\|_1 \\leq 1 \\\\\n",
    "&&& h \\succeq u_{\\text{min}} \\\\\n",
    "&&& h \\preceq u_{\\text{max}}, \n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "In this formulation, we find the holdings vector $h$ which maxmizes the alpha of the resulting portfolio, subject to a number of constraints. In the objective function, we also have a regularization term that penalizes concentrated portfolios.\n",
    "\n",
    "The first constraint is that the predicted risk be less than some maximum limit. The second and third constraints are on the maximum and minumum portfolio factor exposures. The fourth constraint is the \"market neutral constraint: the sum of the weights must be zero. The fifth costraint is the leverage constraint: the sum of the absolute value of the weights must be less than or equal to 1.0. The last are some minimum and maximum limits on indivudual holdings.\n",
    "\n",
    "\n",
    "Another common formumation is to take a pre-defined target weighting $h^*$ (e.g., a quantile portfolio), and solve to get as close to that portfolio while respecting portfolio-level constraints.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{h}{\\text{minimize}}\n",
    "& & \\|h - h^*\\|_2\\\\\n",
    "& \\text{subject to}\n",
    "& & h^T XFX'h + h'Sh \\leq b \\\\\n",
    "&&& X^Th \\preceq k_{\\text{max}} \\\\\n",
    "&&& X^Th \\succeq k_{\\text{min}} \\\\\n",
    "&&& h^T\\mathbb{1} = 0 \\\\\n",
    "&&& \\|h\\|_1 \\leq 1 \\\\\n",
    "&&& h \\succeq u_{\\text{min}} \\\\\n",
    "&&& h \\preceq u_{\\text{max}}, \n",
    "\\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_holdings(\n",
    "    alpha_vector,\n",
    "    risk_model,\n",
    "    risk_cap=0.05,\n",
    "    factor_max=10.0,\n",
    "    factor_min=-10.0,\n",
    "    h_max=0.55,\n",
    "    h_min=-0.55,\n",
    "    lambda_reg=0.0,\n",
    "    obj_max_alpha=True):\n",
    "    \n",
    "    # we need to be very careful to align the index of each item!\n",
    "    try:\n",
    "        F = rm.factor_betas_.loc[alpha_vector.index]\n",
    "        X = rm.factor_cov_matrix_\n",
    "        S = np.diag(rm.idio_var_vector_.loc[alpha_vector.index].values.flatten())\n",
    "        if np.any(np.isnan(S)):\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print(\"Error; likely alphas not in risk model: \" + str(e))\n",
    "\n",
    "    w = cvx.Variable(len(alpha_vector))\n",
    "    f = F.values.T*w\n",
    "    \n",
    "    risk = cvx.quad_form(f, X) + cvx.quad_form(w, S)\n",
    "\n",
    "    if obj_max_alpha:\n",
    "        obj = cvx.Maximize(\n",
    "            alpha_vector.values.flatten()*w - \n",
    "            lambda_reg*cvx.norm(w, 2)\n",
    "        )\n",
    "    else:\n",
    "        # (a - a.mean)/sum(abs(a))\n",
    "        h_star = (alpha_vector.values.flatten()-np.mean(alpha_vector.values.flatten())) \\\n",
    "            /np.sum(np.abs(alpha_vector.values.flatten()))\n",
    "        obj = cvx.Minimize(cvx.norm(w-h_star, 2))\n",
    "\n",
    "    constraints = [\n",
    "        sum(cvx.abs(w)) <= 1.0,\n",
    "        sum(w) == 0.0,\n",
    "        w <= h_max,\n",
    "        w >= h_min,\n",
    "        risk <= risk_cap+risk_cap,\n",
    "        F.values.T*w <= factor_max,\n",
    "        F.values.T*w >= factor_min\n",
    "    ]\n",
    "    \n",
    "    prob = cvx.Problem(obj, constraints)\n",
    "    prob.solve(verbose=True, max_iters=500)\n",
    "\n",
    "    optimal_weights = np.asarray(w.value).flatten()\n",
    "\n",
    "    return pd.DataFrame(data=optimal_weights, index=alpha_vector.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights = find_optimal_holdings(\n",
    "    alpha_vector,\n",
    "    rm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran an optimization with no constraints except for the leverge and net constraint. What did the optimizer do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights.loc[optimal_weights[0].abs()>0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. It put all the weight in just two stocks. We can see that these two stocks have the max and min alpha respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alpha_vector.loc[alpha_vector.alpha_vector==alpha_vector.alpha_vector.max()])\n",
    "print(alpha_vector.loc[alpha_vector.alpha_vector==alpha_vector.alpha_vector.min()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.get_factor_exposures(optimal_weights).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to enforce diversification, we can do a number of things?\n",
    "\n",
    "- add a portfolio risk cap constraint\n",
    "- add a max and min position limit constraint\n",
    "- add a max and min portfolio factor exposure constraint\n",
    "- add regularization to the objective function\n",
    "\n",
    "Question: Can we simply add a constraint that says \"position count must be greater than N securities?\"\n",
    "Answer: This is an integer constraint and is not handled by convex optimizers.\n",
    "\n",
    "So let's try two approaches:\n",
    "\n",
    "### (1) add a regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_1 = find_optimal_holdings(\n",
    "    alpha_vector,\n",
    "    rm,\n",
    "    lambda_reg=5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_1.plot.bar(legend=None, title='Portfolio % Holdings by Stock');\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Well diversified. And we can also look at the net portfolio factor exposures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.get_factor_exposures(optimal_weights_1).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Add Strict Factor Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_2 = find_optimal_holdings(\n",
    "    alpha_vector,\n",
    "    rm,\n",
    "    h_max=0.02,\n",
    "    h_min=-0.02,\n",
    "    risk_cap=0.0015,\n",
    "    factor_max=0.015,\n",
    "    factor_min=-0.015,\n",
    "    obj_max_alpha=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_2.plot.bar(legend=None, title='Portfolio % Holdings by Stock');\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you compare and constrast these two approaches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm.get_factor_exposures(optimal_weights_2).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnd-quant-basics",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
